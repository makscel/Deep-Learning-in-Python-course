{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_agzR4AoBkm",
        "outputId": "7dba9ff0-19cc-41f0-81eb-4de1d7995e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Porównanie dwóch klasyfikatorów ===\n",
            "LogisticRegression -> Accuracy: 0.9286 | Macro-F1: 0.9246\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95        21\n",
            "           1       1.00      0.71      0.83         7\n",
            "           2       1.00      1.00      1.00         7\n",
            "           3       0.90      0.90      0.90        10\n",
            "           4       1.00      0.85      0.92        13\n",
            "           5       0.91      1.00      0.95        10\n",
            "           6       1.00      1.00      1.00         8\n",
            "           7       0.73      1.00      0.84         8\n",
            "\n",
            "    accuracy                           0.93        84\n",
            "   macro avg       0.94      0.93      0.92        84\n",
            "weighted avg       0.94      0.93      0.93        84\n",
            "\n",
            "------------------------------------------------------------\n",
            "RandomForest -> Accuracy: 0.7976 | Macro-F1: 0.7755\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.95      0.87        21\n",
            "           1       0.60      0.43      0.50         7\n",
            "           2       0.88      1.00      0.93         7\n",
            "           3       0.67      0.40      0.50        10\n",
            "           4       0.69      0.69      0.69        13\n",
            "           5       0.83      1.00      0.91        10\n",
            "           6       1.00      1.00      1.00         8\n",
            "           7       0.86      0.75      0.80         8\n",
            "\n",
            "    accuracy                           0.80        84\n",
            "   macro avg       0.79      0.78      0.78        84\n",
            "weighted avg       0.79      0.80      0.78        84\n",
            "\n",
            "\n",
            ">>> Lepszy wg Macro-F1: LogisticRegression\n"
          ]
        }
      ],
      "source": [
        "################# zadanie na 3 ##################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "DATA_PATH = \"emvic.data\"\n",
        "df = pd.read_csv(DATA_PATH, sep=\"\\t\", header=None)\n",
        "\n",
        "y = df.iloc[:, 0]         # etykiety (aXX)\n",
        "X = df.iloc[:, 1:]        # cechy\n",
        "\n",
        "# zamiana etykiet na wartosci liczbowe + podział na train/test ---\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
        ")\n",
        "\n",
        "# dwa klasyfikatory\n",
        "# Logistic Regression\n",
        "clf_lr = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000, n_jobs=-1, random_state=42)\n",
        ")\n",
        "\n",
        "# Random Forest\n",
        "clf_rf = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Trenowanie\n",
        "clf_lr.fit(X_train, y_train)\n",
        "clf_rf.fit(X_train, y_train)\n",
        "\n",
        "# Predykcje i metryki\n",
        "pred_lr = clf_lr.predict(X_test)\n",
        "pred_rf = clf_rf.predict(X_test)\n",
        "\n",
        "def report(name, y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    print(f\"{name} -> Accuracy: {acc:.4f} | Macro-F1: {f1m:.4f}\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "\n",
        "print(\"=== Porównanie dwóch klasyfikatorów ===\")\n",
        "report(\"LogisticRegression\", y_test, pred_lr)\n",
        "print(\"-\"*60)\n",
        "report(\"RandomForest\",      y_test, pred_rf)\n",
        "\n",
        "# zwycięzca na podstawie Macro-F1\n",
        "f1_lr = f1_score(y_test, pred_lr, average=\"macro\")\n",
        "f1_rf = f1_score(y_test, pred_rf, average=\"macro\")\n",
        "winner = \"LogisticRegression\" if f1_lr >= f1_rf else \"RandomForest\"\n",
        "print(f\"\\n>>> Lepszy wg Macro-F1: {winner}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUlsC89t-ESr",
        "outputId": "e4143c10-85c5-4c9c-a7ac-8c0fae6122b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Wyniki holdout (test) ===\n",
            "       model  accuracy  macro_f1\n",
            "      LogReg  0.714286  0.689563\n",
            "   LinearSVC  0.702381  0.679523\n",
            "  ExtraTrees  0.666667  0.664504\n",
            "RandomForest  0.642857  0.623252\n",
            "   GradBoost  0.607143  0.530732\n",
            "     SVC_RBF  0.571429  0.483228\n",
            "         KNN  0.404762  0.337260\n",
            "\n",
            "=== 5-fold CV dla TOP-3 (bez selekcji) ===\n",
            "LogReg        ACC 0.7113±0.0508 | F1m 0.6894±0.0630\n",
            "LinearSVC     ACC 0.6970±0.0663 | F1m 0.6753±0.0771\n",
            "ExtraTrees    ACC 0.6754±0.0386 | F1m 0.6420±0.0539\n",
            "\n",
            "=== 5-fold CV dla TOP-3 z selekcją cech (k=300) ===\n",
            "LogReg        ACC 0.7764±0.0166 | F1m 0.7573±0.0190\n",
            "LinearSVC     ACC 0.7477±0.0231 | F1m 0.7226±0.0226\n",
            "ExtraTrees    ACC 0.6996±0.0257 | F1m 0.6561±0.0097\n"
          ]
        }
      ],
      "source": [
        "############### zadanie na 4.5 ##################\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "RNG = 42\n",
        "\n",
        "# Wczytanie i rozcięcie na bloki 2048\n",
        "df = pd.read_csv(\"emvic.data\", sep=\"\\t\", header=None)\n",
        "sid = df.iloc[:, 0].astype(str)\n",
        "\n",
        "def blk(i):  # i=0..5 -> sx,sy,lx,rx,ly,ry\n",
        "    start = 1 + i*2048\n",
        "    return slice(start, start+2048)\n",
        "\n",
        "sx = df.iloc[:, blk(0)].to_numpy(dtype=float)\n",
        "sy = df.iloc[:, blk(1)].to_numpy(dtype=float)\n",
        "lx = df.iloc[:, blk(2)].to_numpy(dtype=float)\n",
        "rx = df.iloc[:, blk(3)].to_numpy(dtype=float)\n",
        "ly = df.iloc[:, blk(4)].to_numpy(dtype=float)\n",
        "ry = df.iloc[:, blk(5)].to_numpy(dtype=float)\n",
        "\n",
        "# PRZYGOTOWANIE DANYCH\n",
        "# cechy różnicowe (blad patrzenia - to jak oczy podazaja za bodzcem)\n",
        "dxL, dyL = lx - sx, ly - sy\n",
        "dxR, dyR = rx - sx, ry - sy\n",
        "\n",
        "# Odrzucamy surowe sx, sy (po wykorzystaniu do różnic niepotrzebne do klasyfikacji)\n",
        "# sklejenie cech: 4 * 2048\n",
        "X = np.hstack([dxL, dyL, dxR, dyR])\n",
        "X = pd.DataFrame(X)\n",
        "\n",
        "# usuniecie kolumn stalych i silnie skorelowanych (redukcja wymiaru bez straty informacji)\n",
        "X = pd.DataFrame(VarianceThreshold(0.0).fit_transform(X))  # stałe out\n",
        "corr = X.corr().abs()\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "to_drop = [c for c in upper.columns if any(upper[c] > 0.98)]\n",
        "if to_drop:\n",
        "    X = X.drop(columns=to_drop)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(sid)\n",
        "\n",
        "# trenowanie\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RNG\n",
        ")\n",
        "\n",
        "# modele\n",
        "CLFS = {\n",
        "    \"LogReg\":      (LogisticRegression(max_iter=1000, n_jobs=-1, random_state=RNG), True),\n",
        "    \"SVC_RBF\":     (SVC(kernel=\"rbf\", random_state=RNG), True),\n",
        "    \"LinearSVC\":   (LinearSVC(max_iter=10000, tol=1e-3, random_state=RNG), True),\n",
        "    \"KNN\":         (KNeighborsClassifier(n_neighbors=7), True),\n",
        "    \"RandomForest\":(RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RNG), False),\n",
        "    \"ExtraTrees\":  (ExtraTreesClassifier(n_estimators=300, n_jobs=-1, random_state=RNG), False),\n",
        "    \"GradBoost\":   (GradientBoostingClassifier(random_state=RNG), False)\n",
        "}\n",
        "\n",
        "def make_pipe(estimator, need_scale, with_fs=False, k=None):\n",
        "    steps = [(\"impute\", SimpleImputer(strategy=\"median\"))]\n",
        "    if need_scale:\n",
        "        steps += [(\"scale\", StandardScaler())]\n",
        "    if with_fs:\n",
        "        steps += [(\"select\", SelectKBest(score_func=f_classif, k=k))]\n",
        "    steps += [(\"clf\", estimator)]\n",
        "    return Pipeline(steps)\n",
        "\n",
        "# 7 klasyfikatorow, ranking po Macro-F1\n",
        "rows = []\n",
        "for name, (est, sc) in CLFS.items():\n",
        "    pipe = make_pipe(est, sc, with_fs=False)\n",
        "    pipe.fit(X_train, y_train)\n",
        "    pred = pipe.predict(X_test)\n",
        "    rows.append((name,\n",
        "                 accuracy_score(y_test, pred),\n",
        "                 f1_score(y_test, pred, average=\"macro\")))\n",
        "res = pd.DataFrame(rows, columns=[\"model\",\"accuracy\",\"macro_f1\"]).sort_values(\n",
        "    by=[\"macro_f1\",\"accuracy\"], ascending=False\n",
        ")\n",
        "print(\"=== Wyniki holdout (test) ===\")\n",
        "print(res.to_string(index=False))\n",
        "\n",
        "# CV dla top3\n",
        "top3 = res.head(3)[\"model\"].tolist()\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
        "print(\"\\n=== 5-fold CV dla TOP-3 (bez selekcji) ===\")\n",
        "for name in top3:\n",
        "    est, sc = CLFS[name]\n",
        "    pipe = make_pipe(est, sc, with_fs=False)\n",
        "    scores = cross_validate(pipe, X, y, scoring={\"acc\":\"accuracy\",\"f1m\":\"f1_macro\"},\n",
        "                            cv=cv, n_jobs=-1, return_train_score=False)\n",
        "    print(f\"{name:12s}  ACC {scores['test_acc'].mean():.4f}±{scores['test_acc'].std():.4f} | \"\n",
        "          f\"F1m {scores['test_f1m'].mean():.4f}±{scores['test_f1m'].std():.4f}\")\n",
        "\n",
        "# selekcja cech i ponownie CV dla top3\n",
        "k_sel = min(300, X.shape[1])\n",
        "print(f\"\\n=== 5-fold CV dla TOP-3 z selekcją cech (k={k_sel}) ===\")\n",
        "for name in top3:\n",
        "    est, sc = CLFS[name]\n",
        "    pipe = make_pipe(est, sc, with_fs=True, k=k_sel)\n",
        "    scores = cross_validate(pipe, X, y, scoring={\"acc\":\"accuracy\",\"f1m\":\"f1_macro\"},\n",
        "                            cv=cv, n_jobs=-1, return_train_score=False)\n",
        "    print(f\"{name:12s}  ACC {scores['test_acc'].mean():.4f}±{scores['test_acc'].std():.4f} | \"\n",
        "          f\"F1m {scores['test_f1m'].mean():.4f}±{scores['test_f1m'].std():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}